{"root":{"data":{"id":"dg0qnjivvvs0","created":1769655878724,"text":"From Inputs to Mechanisms: A Survey of Multi-Level Interpretability in Vision-Language Models","expandState":"expand"},"children":[{"data":{"id":"dg0qoa3j1vk0","created":1769655936569,"text":"Preliminaries(§2)","layout":null,"expandState":"expand"},"children":[{"data":{"id":"dg0qooav0200","created":1769655967487,"text":"Architecture of Vision-Language Models(§2.1)","expandState":"expand","layout":null},"children":[{"data":{"id":"dg0qphm2o280","created":1769656031292,"text":"Visual Encoder(§2.1.1); Pre-trained Large Language Model(§2.1.2); Learnable Adapters for Vision-Language Alignment(§2.1.3)","layout":null,"expandState":"expand"},"children":[]}]},{"data":{"id":"dg0qp3zdu5s0","created":1769656001622,"text":"Interpretability Granularity in Vision-Language Models(§2.2)","layout":null,"expandState":"expand"},"children":[{"data":{"id":"dg0qqi47uwo0","created":1769656110753,"text":"Input-Output Level(§2.2.1); Representational Level(§2.2.2); Mechanistic Level(§2.2.3)","layout":null,"expandState":"expand"},"children":[]}]}]},{"data":{"id":"dg0quvn41n40","created":1769656453650,"text":"Taxonomy(§3)","layout":null,"expandState":"expand"},"children":[{"data":{"id":"dg0qvyqfiao0","created":1769656538745,"text":"Input-Output Level(§3.1)","layout":null,"expandState":"expand"},"children":[{"data":{"id":"dg0qwmml89c0","created":1769656590756,"text":"Perturbation-based Explanation","layout":null,"expandState":"expand"},"children":[]},{"data":{"id":"dg0qwuzh1u00","created":1769656608949,"text":"Example-based Explanation","layout":null,"expandState":"expand"},"children":[]},{"data":{"id":"dg0qwvt0v1c0","created":1769656610736,"text":"Surrogate Model(21)","layout":null,"expandState":"collapse"},"children":[{"data":{"id":"dg0wyk47xfs0","created":1769673668679,"text":"Others\n\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier（LIME）\nTransLIME: Towards transfer explainability to explain black-box models on tabular datasets\nAnchors: High-Precision Model-Agnostic Explanations（Anchors）\nLemna: Explaining deep learning based security applications（Lemna）\nFactual and Counterfactual Explanations for Black Box Decision Making（LORE）\nEnhancing decision tree based interpretation of deep neural networks through L1-orthogonal regularization（Decision Tree）\nFaithful and customizable explanations of black box models（MUSE）\nMANE: Model-agnostic non-linear explanations for deep learning model（MANE）\nA Unified Approach to Interpreting Model Predictions（SHAP）\nImproving the Weighting Strategy in KernelSHAP（KernelSHAP）\n\n\nSurvey\nWhich LIME should I trust? Concepts, Challenges, and Solutions\n\n\nLLM\nBERT meets Shapley: Extending SHAP Explanations to Transformer-based Classifiers（TransSHAP）\nLearning to Explain: Prototype-Based Surrogate Models for LLM Classification\nLarge Language Models as Surrogate Models in Evolutionary Algorithms: A Preliminary Study\nHarnessing LLMs Explanations to Boost Surrogate Models in Tabular Data Classification\nTokenSHAP: Interpreting Large Language Models with Monte Carlo Shapley Value Estimation\nIntegration of Explainable AI Techniques with Large Language Models for Enhanced Interpretability for Sentiment Analysis\n\n\nMLLM\nDIME: Fine-grained Interpretations of Multimodal Models via Disentangled Local Explanations（DIME）\nVALE: A Multimodal Visual and Language Explanation Framework for Image Classifiers using eXplainable AI and Language Models（SHAP）\nMM-SHAP: A Performance-agnostic Metric for Measuring Multimodal Contributions in Vision and Language Models & Tasks\nAttention, Please! PixelSHAP Reveals What Vision-Language Models Actually Focus On"},"children":[]}]},{"data":{"id":"dg0qxi3lym00","created":1769656659265,"text":"Explanation Model","layout":null,"expandState":"expand"},"children":[]},{"data":{"id":"dg0qxm94zb40","created":1769656668307,"text":"Chain of Thought","layout":null,"expandState":"expand"},"children":[]}]},{"data":{"id":"dg0qvznm39s0","created":1769656540752,"text":"Representational Level(§3.2)","layout":null,"expandState":"expand"},"children":[{"data":{"id":"dg0qwwqpvsg0","created":1769656612773,"text":"Representation Engineering","layout":null,"expandState":"expand"},"children":[]},{"data":{"id":"dg0qwxt8w4o0","created":1769656615103,"text":"Concept-based Explanation","layout":null,"expandState":"expand"},"children":[]},{"data":{"id":"dg0qwyvykso0","created":1769656617444,"text":"Probing-based Explanation","layout":null,"expandState":"expand"},"children":[]}]},{"data":{"id":"dg0qw0p7gog0","created":1769656543025,"text":"Mechanistic Level(§3.3)","layout":null,"expandState":"expand"},"children":[{"data":{"id":"dg0qwzr6o1s0","created":1769656619332,"text":"Attention-based Explanation(18)","layout":null,"expandState":"collapse"},"children":[{"data":{"id":"dg0wzh739ls0","created":1769673740687,"text":"LLM\nAttentionViz: A Global View of Transformer Attention（跨序列、跨注意力头的全局模式可视化）\nQuantifying Attention Flow in Transformers\nGeneralized Attention Flow: Feature Attribution for Transformer Models via Maximum Flow\nUnderstanding stance classification of BERT models: an attention-based framework\n\n\nMLLM/VLM\nLVLM-Intrepret: An Interpretability Tool for Large Vision-Language Models（交互可解释性工具平台）\nFrom Redundancy to Relevance: Information Flow in LVLMs Across Reasoning Tasks\nCross-modal Information Flow in Multimodal Large Language Models（主动阻断特定的注意力边（例如阻止“问题 Token”关注“图像 Token”），通过观察模型输出概率的变化来反向推导信息是如何流动 ）\nToken Activation Map to Visually Explain Multimodal LLMs（TAM）\nMeasuring the mixing of contextual information in the transformer（用类似于注意力展平的方式计算Transformer编码器中的标记贡献矩阵。没有使用原始注意力权重，而是使用线性变换后的输入和输出向量之间的曼哈顿距离来量化贡献）\nShallow Focus, Deep Fixes: Enhancing Shallow Layers Vision Attention Sinks to Alleviate Hallucination in LVLMs（本文分析了图像标记在模型中层次和注意力头的分布，揭示了一个有趣但常见的现象：大多数幻觉与图像标记注意力矩阵的注意力消耗模式密切相关）\nIntegrating attention into explanation frameworks for language and vision transformers（基于标准基准测试及与广泛使用的解释方法的对比研究的实证评估表明，注意力权重可以有意义地纳入所研究的 XAI 框架中，突出其在丰富变换器可解释性的价值）\n\n\nSurvey\nEfficient Attention Mechanisms for Large Language Models: A Survey（主要关于高效注意力机制，不太相关）\nAttention Heads of Large Language Models: A Survey\nAttention, please! A survey of Neural Attention Models in Deep Learning\n\n\nOthers\nViT-NeT: Interpretable Vision Transformers with Neural Tree Decoder（涉及可解释性的部分主要是定性评估） Interpretability-aware vision transformer\nVisualizing and Understanding Patch Interactions in Vision Transformer\nGeneric Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers（多模态transformer，不仅利用了注意力权重，还结合了梯度来过滤噪声并确定不同注意力头的重要性）\nDemystify Self-Attention in Vision Transformers from a Semantic Perspective: Analysis and Application（ViT，引入了传统的图像处理技术 SIFT（尺度不变特征变换），将低级像素映射到具有丰富语义信息的中级特征空间）"},"children":[]}]},{"data":{"id":"dg0qx20wwj40","created":1769656624274,"text":"Neuron-based Explanation(15)","layout":null,"expandState":"collapse"},"children":[{"data":{"id":"dg0x0g40ugg0","created":1769673816689,"text":"Survey\nNeuron-level Interpretation of Deep NLP Models: A Survey（metric）\n\n\nLLM\nNeuronScope: A Multi-Agent Framework for Explaining Polysemantic Neurons in Language Models（Neuron-based）\nKnowledge neurons in pretrained transformers\nIRCAN: mitigating knowledge conflicts in LLM generation via identifying and reweighting context-aware neurons\nDeepDecipher: Accessing and Investigating Neuron Activation in Large Language Models（Neuron-based）\nNeuron to Graph (N2G): Interpreting Language Model Neurons at Scale（Neuron-based）\n\n\nMLLM\nMMNeuron: Discovering Neuron-Level Domain-Specific Interpretation in Multimodal Large Language Model（Neuron-based）\nFinding and editing multi-modal neurons in pre-trained transformers（Neuron-based）\nTowards Neuron Attributions in Multi-Modal Large Language Models (NAM)\nMultimodal neurons in artificial neural networks（Neuron-based）\nMultimodal neurons in pretrained text-only transformers（Neuron-based）\nMiner: Mining the underlying pattern of modality-specific neurons in multimodal large language models（Neuron-based）\nInterpreting the second-order effects of neurons in clip（Neuron-based）\nIdentifying Multi-modal Knowledge Neurons in Pretrained Transformers via Two-stage Filtering\nDeciphering Functions of Neurons in Vision-Language Models"},"children":[]}]},{"data":{"id":"dg0qx3kbvwg0","created":1769656627625,"text":"Decomposition-based Explanation(15)","layout":null,"expandState":"collapse"},"children":[{"data":{"id":"dg0wwvnctgg0","created":1769673537053,"text":"LLM\nAttention-Aware Layer-Wise Relevance Propagation for Transformers\nXai for transformers: Better explanations through conservative propagation\nTransformer interpretability beyond attention visualization\n\n\nMLLM\nLayer-Wise Modality Decomposition for Interpretable Multimodal Sensor Fusion\nInterpreting clip’s image representation via text-based decomposition\nExplaining Similarity in Vision-Language Encoders with Weighted Banzhaf Interactions（博弈论方法倒推贡献，黑盒）\nResiDual Transformer Alignment with Spectral Decomposition（在CLIP上对残差单元进行谱分解)\nQuantifying Structure in CLIP Embeddings: A Statistical Framework for Concept Interpretation（将嵌入矩阵分解为载荷（loadings）和概念向量）\nDecomposing and Interpreting Image Representations via Text in ViTs Beyond CLIP\nInterpreting ResNet-based CLIP via Neuron-Attention Decomposition 将模型输出直接分摊到内部组件（神经元、Attention Head）和输入维度（Token/Patch）\n\n\nOthers\nLayer-wise Relevance Propagation for Neural Networks with Local Renormalization Layers（LRP）\nVisualizing and understanding neural machine translation（LRP）\nExplaining nonlinear classification decisions with deep taylor decomposition（DTD）\nInterpretability in Parameter Space: Minimizing Mechanistic Description Length with Attribution-based Parameter Decomposition（APD，仅适用于toy models）\nQuantifying Model Complexity via Functional Decomposition for Better Post-hoc Interpretability（metric）"},"children":[]}]},{"data":{"id":"dg0qx2t5bs80","created":1769656625981,"text":"Gradient-based Explanation(23)","layout":null,"expandState":"collapse"},"children":[{"data":{"id":"dg0w20ymt5s0","created":1769671119330,"text":"Others\nAxiomatic Attribution for Deep Networks (Integrated Gradients)\nDiscretized Integrated Gradients for Explaining Language Models（DIG）\nGuided integrated gradients: An adaptive path method for removing noise\nGrad-CAM: Visual Explanatio ns from Deep Networks via Gradient-based Localization（Grad-CAM）\nGrad-CAM++: Generalized Gradient-Based Visual Explanations for Deep Convolutional Networks（Grad-CAM++）\nSmooth Grad-CAM++: An Enhanced Inference Level Visualization Technique for Deep Convolutional Neural Network Models\nSmoothGrad: removing noise by adding noise（SmoothGrad）\n\n\nSurvey\nGradient based Feature Attribution in Explainable AI: A Technical Review（metric）\n\n\nLLM\nGrad-ELLM: Gradient-based Explanations for Decoder-only LLMs\nSequential Integrated Gradients: a simple but effective method for explaining language models\nAnalyzing Chain-of-Thought Prompting in Large Language Models via Gradient-based Feature Attributions\nDecoding Layer Saliency in Language Transformers\nWhat happened in LLMs layers when trained for fast vs. slow thinking: A gradient perspective\nEnhancing large language model performance with gradient-based parameter selection\n\n\nMLLM\nQ-GroundCAM: Quantifying Grounding in Vision Language Models via GradCAM\nGradient-based Visual Explanation for Transformer-based CLIP\nGrad-ECLIP: Gradient-based Visual and Textual Explanations for CLIP\nImproving Visual Grounding by Encouraging Consistent Gradient-based Explanations\nFrom Redundancy to Relevance: Information Flow in LVLMs Across Reasoning Tasks（LLaVA-CAM + attention score）\nMitigating Multimodal Hallucinations via Gradient-based Self-Reflection\nGLIMPSE: Holistic Cross-Modal Explainability for Large Vision-Language Models\nGrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs and VLMs\nMulti-Modal Interpretability for Enhanced Localization in Vision-Language Models"},"children":[]}]},{"data":{"id":"dg0qx4gy1aw0","created":1769656629597,"text":"Logit Lens","layout":null,"expandState":"expand"},"children":[{"data":{"id":"dg0x0t8hryg0","created":1769673845257,"text":"分支主题"},"children":[]}]},{"data":{"id":"dg0qx5mirz40","created":1769656632111,"text":"Sparse Autoencoders","layout":null,"expandState":"expand"},"children":[]},{"data":{"id":"dg0qx6ntm4g0","created":1769656634366,"text":"Causal Tracing","layout":null,"expandState":"expand"},"children":[]},{"data":{"id":"dg0qx7h3dko0","created":1769656636136,"text":"Circuit Discovery","layout":null,"expandState":"expand"},"children":[]}]}]},{"data":{"id":"dg0spmhtaao0","created":1769661684138,"text":"Evaluation(§4)","layout":null,"expandState":"expand"},"children":[{"data":{"id":"dg0sqc2kfps0","created":1769661739812,"text":"Evaluation Criteria","layout":null,"expandState":"expand"},"children":[{"data":{"id":"dg0sqljhfyo0","created":1769661760426,"text":"Faithfulness","layout":null,"expandState":"expand"},"children":[]},{"data":{"id":"dg0ss1tbkz40","created":1769661874214,"text":"Stability","layout":null,"expandState":"expand"},"children":[]},{"data":{"id":"dg0ss5flp8g0","created":1769661882091,"text":"Efficiency","layout":null,"expandState":"expand"},"children":[]}]},{"data":{"id":"dg0sqfsu50g0","created":1769661747931,"text":"Benchmarks","layout":null,"expandState":"expand"},"children":[]},{"data":{"id":"dg0stet6cts0","created":1769661980867,"text":"Toolkits","layout":null,"expandState":"expand"},"children":[]}]},{"data":{"id":"dg0qv1divzk0","created":1769656466131,"text":"Applications(§5)","layout":null,"expandState":"expand"},"children":[{"data":{"id":"dg0qzxz97b40","created":1769656850559,"text":"Medical Vision-Language Systems","layout":null,"expandState":"expand"},"children":[]},{"data":{"id":"dg0qzz9nqp40","created":1769656853365,"text":"Jailbreak and Safety Analysis","layout":null,"expandState":"expand"},"children":[]},{"data":{"id":"dg0r015vm9k0","created":1769656857490,"text":"Harmful Content Detection","layout":null,"expandState":"expand"},"children":[]},{"data":{"id":"dg0sudzqdrs0","created":1769662057451,"text":"Lessons from Applications","layout":null,"expandState":"expand"},"children":[]}]},{"data":{"id":"dg0qnudnka00","created":1769655902352,"text":"Challenges and Future Directions(§6)","layout":null,"expandState":"expand"},"children":[{"data":{"id":"dg0sv6578sw0","created":1769662118732,"text":"Standardization of Evaluation; Interpretable Multimodal Agents","layout":null,"expandState":"expand"},"children":[]}]}]},"template":"right","theme":"fresh-blue","version":"1.4.43"}