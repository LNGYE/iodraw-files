{"root":{"data":{"id":"dg0qnjivvvs0","created":1769655878724,"text":"From Inputs to Mechanisms: A Survey of Multi-Level Interpretability in Vision-Language Models","expandState":"expand","font-weight":"bold"},"children":[{"data":{"id":"dg0qoa3j1vk0","created":1769655936569,"text":"Preliminaries(§2)","layout":null,"expandState":"expand","font-weight":"bold"},"children":[{"data":{"id":"dg0qooav0200","created":1769655967487,"text":"Architecture of Vision-Language Models(§2.1)","expandState":"expand","layout":null},"children":[{"data":{"id":"dg0qphm2o280","created":1769656031292,"text":"Visual Encoder(§2.1.1); Pre-trained Large Language Model(§2.1.2); Learnable Adapters for Vision-Language Alignment(§2.1.3)","layout":null,"expandState":"expand"},"children":[]}]},{"data":{"id":"dg0qp3zdu5s0","created":1769656001622,"text":"Interpretability Granularity in Vision-Language Models(§2.2)","layout":null,"expandState":"expand"},"children":[{"data":{"id":"dg0qqi47uwo0","created":1769656110753,"text":"Input-Output Level(§2.2.1); Representational Level(§2.2.2); Mechanistic Level(§2.2.3)","layout":null,"expandState":"expand"},"children":[]}]}]},{"data":{"id":"dg0quvn41n40","created":1769656453650,"text":"Taxonomy(§3)","layout":null,"expandState":"expand","font-weight":"bold"},"children":[{"data":{"id":"dg0qvyqfiao0","created":1769656538745,"text":"Input-Output Level(§3.1)","layout":null,"expandState":"expand"},"children":[{"data":{"id":"dg0qwmml89c0","created":1769656590756,"text":"Perturbation-based Explanation","layout":null,"expandState":"expand"},"children":[]},{"data":{"id":"dg0qwuzh1u00","created":1769656608949,"text":"Example-based Explanation","layout":null,"expandState":"expand"},"children":[]},{"data":{"id":"dg0qwvt0v1c0","created":1769656610736,"text":"Surrogate Model(21)","layout":null,"expandState":"collapse"},"children":[{"data":{"id":"dg0wyk47xfs0","created":1769673668679,"text":"Others\n\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier（LIME）\nTransLIME: Towards transfer explainability to explain black-box models on tabular datasets\nAnchors: High-Precision Model-Agnostic Explanations（Anchors）\nLemna: Explaining deep learning based security applications（Lemna）\nFactual and Counterfactual Explanations for Black Box Decision Making（LORE）\nEnhancing decision tree based interpretation of deep neural networks through L1-orthogonal regularization（Decision Tree）\nFaithful and customizable explanations of black box models（MUSE）\nMANE: Model-agnostic non-linear explanations for deep learning model（MANE）\nA Unified Approach to Interpreting Model Predictions（SHAP）\nImproving the Weighting Strategy in KernelSHAP（KernelSHAP）\n\n\nSurvey\nWhich LIME should I trust? Concepts, Challenges, and Solutions\n\n\nLLM\nBERT meets Shapley: Extending SHAP Explanations to Transformer-based Classifiers（TransSHAP）\nLearning to Explain: Prototype-Based Surrogate Models for LLM Classification\nLarge Language Models as Surrogate Models in Evolutionary Algorithms: A Preliminary Study\nHarnessing LLMs Explanations to Boost Surrogate Models in Tabular Data Classification\nTokenSHAP: Interpreting Large Language Models with Monte Carlo Shapley Value Estimation\nIntegration of Explainable AI Techniques with Large Language Models for Enhanced Interpretability for Sentiment Analysis\n\n\nMLLM\nDIME: Fine-grained Interpretations of Multimodal Models via Disentangled Local Explanations（DIME）\nVALE: A Multimodal Visual and Language Explanation Framework for Image Classifiers using eXplainable AI and Language Models（SHAP）\nMM-SHAP: A Performance-agnostic Metric for Measuring Multimodal Contributions in Vision and Language Models & Tasks\nAttention, Please! PixelSHAP Reveals What Vision-Language Models Actually Focus On","layout":null},"children":[]}]},{"data":{"id":"dg0qxi3lym00","created":1769656659265,"text":"Explanation Model(8)","layout":null,"expandState":"collapse"},"children":[{"data":{"id":"dg0x38ec3200","created":1769674034991,"text":"LLM\nFew-Shot Multimodal Explanation for Visual Question Answering\nExplain Yourself! Leveraging Language Models for Commonsense Reasoning\n\n\nMLLM/VLM\nZero-Shot Textual Explanations via Translating Decision-Critical Features\n\n\nOthers\nMultimodal Explanations: Justifying Decisions and Pointing to the Evidence\nFaithful Multimodal Explanation for Visual Question Answering\nFaithful Attention Explainer: Verbalizing Decisions Based on Discriminative Features\nDEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision Models\nFaithful and Plausible Natural Language Explanations for Image Classification: A Pipeline Approach","layout":null},"children":[]}]},{"data":{"id":"dg0qxm94zb40","created":1769656668307,"text":"Chain of Thought","layout":null,"expandState":"expand"},"children":[]}]},{"data":{"id":"dg0qvznm39s0","created":1769656540752,"text":"Representational Level(§3.2)","layout":null,"expandState":"expand"},"children":[{"data":{"id":"dg0qwwqpvsg0","created":1769656612773,"text":"Representation Engineering(23)","layout":null,"expandState":"collapse"},"children":[{"data":{"id":"dg0x2n5reps0","created":1769673988760,"text":"Survey\nTaxonomy, Opportunities, and Challenges of Representation Engineering for Large Language Models（metric）\nRepresentation Engineering for Large-Language Models: Survey and Research Challenges\n\n\nLLM\nRepresentation Engineering: A Top-Down Approach to AI Transparency\nRefusal in Language Models Is Mediated by a Single Direction\nUnderstanding Jailbreak Success: A Study of Latent Space Dynamics in Large Language Models\nSurgical, Cheap, and Flexible: Mitigating False Refusal in Language Models via Single Vector Ablation\nReFT: Representation Finetuning for Language Models\nImproving Instruction-Following in Language Models through Activation Steering\nRepIt: Steering Language Models with Concept-Specific Refusal Vectors\nPersonalized Steering of Large Language Models: Versatile Steering Vectors Through Bi-directional Preference Optimization\nSpectral editing of activations for large language model alignment\nSteering large language models using conceptors: Improving addition-based activation engineering\nRepresentation surgery: Theory and practice of affine steering\n\n\nMLLM\nWhy Representation Engineering Works: A Theoretical and Empirical Study in Vision-Language Models\nReducing Hallucinations in Vision-Language Models via Latent Space Steering\nSteering Away from Harm: An Adaptive Approach to Defending Vision Language Model Against Jailbreaks\nDo we Really Need Visual Instructions? Towards Visual Instruction-Free Fine-tuning for Large Vision-Language Models\nEffective and Efficient Adversarial Detection for Vision-Language Models via A Single Vector\nSelf-Aware Safety Augmentation: Leveraging Internal Semantic Understanding to Enhance Safety in Vision-Language Models\nThe Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering\nRe-Imagining Multimodal Instruction Tuning: A Representation View\nEvaluating and Steering Modality Preferences in Multimodal Large Language Model\nARGUS: Defending Against Multimodal Indirect Prompt Injection via Steering Instruction-Following Behavior","layout":null},"children":[]}]},{"data":{"id":"dg0qwxt8w4o0","created":1769656615103,"text":"Concept-based Explanation","layout":null,"expandState":"expand"},"children":[]},{"data":{"id":"dg0qwyvykso0","created":1769656617444,"text":"Probing-based Explanation","layout":null,"expandState":"expand"},"children":[]}]},{"data":{"id":"dg0qw0p7gog0","created":1769656543025,"text":"Mechanistic Level(§3.3)","layout":null,"expandState":"expand"},"children":[{"data":{"id":"dg0qwzr6o1s0","created":1769656619332,"text":"Attention-based Explanation(18)","layout":null,"expandState":"collapse"},"children":[{"data":{"id":"dg0wzh739ls0","created":1769673740687,"text":"LLM\nAttentionViz: A Global View of Transformer Attention（跨序列、跨注意力头的全局模式可视化）\nQuantifying Attention Flow in Transformers\nGeneralized Attention Flow: Feature Attribution for Transformer Models via Maximum Flow\nUnderstanding stance classification of BERT models: an attention-based framework\n\n\nMLLM/VLM\nLVLM-Intrepret: An Interpretability Tool for Large Vision-Language Models（交互可解释性工具平台）\nFrom Redundancy to Relevance: Information Flow in LVLMs Across Reasoning Tasks\nCross-modal Information Flow in Multimodal Large Language Models（主动阻断特定的注意力边（例如阻止“问题 Token”关注“图像 Token”），通过观察模型输出概率的变化来反向推导信息是如何流动 ）\nToken Activation Map to Visually Explain Multimodal LLMs（TAM）\nMeasuring the mixing of contextual information in the transformer（用类似于注意力展平的方式计算Transformer编码器中的标记贡献矩阵。没有使用原始注意力权重，而是使用线性变换后的输入和输出向量之间的曼哈顿距离来量化贡献）\nShallow Focus, Deep Fixes: Enhancing Shallow Layers Vision Attention Sinks to Alleviate Hallucination in LVLMs（本文分析了图像标记在模型中层次和注意力头的分布，揭示了一个有趣但常见的现象：大多数幻觉与图像标记注意力矩阵的注意力消耗模式密切相关）\nIntegrating attention into explanation frameworks for language and vision transformers（基于标准基准测试及与广泛使用的解释方法的对比研究的实证评估表明，注意力权重可以有意义地纳入所研究的 XAI 框架中，突出其在丰富变换器可解释性的价值）\n\n\nSurvey\nEfficient Attention Mechanisms for Large Language Models: A Survey（主要关于高效注意力机制，不太相关）\nAttention Heads of Large Language Models: A Survey\nAttention, please! A survey of Neural Attention Models in Deep Learning\n\n\nOthers\nViT-NeT: Interpretable Vision Transformers with Neural Tree Decoder（涉及可解释性的部分主要是定性评估） Interpretability-aware vision transformer\nVisualizing and Understanding Patch Interactions in Vision Transformer\nGeneric Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers（多模态transformer，不仅利用了注意力权重，还结合了梯度来过滤噪声并确定不同注意力头的重要性）\nDemystify Self-Attention in Vision Transformers from a Semantic Perspective: Analysis and Application（ViT，引入了传统的图像处理技术 SIFT（尺度不变特征变换），将低级像素映射到具有丰富语义信息的中级特征空间）","layout":null},"children":[]}]},{"data":{"id":"dg0qx20wwj40","created":1769656624274,"text":"Neuron-based Explanation(15)","layout":null,"expandState":"collapse"},"children":[{"data":{"id":"dg0x0g40ugg0","created":1769673816689,"text":"Survey\nNeuron-level Interpretation of Deep NLP Models: A Survey（metric）\n\n\nLLM\nNeuronScope: A Multi-Agent Framework for Explaining Polysemantic Neurons in Language Models（Neuron-based）\nKnowledge neurons in pretrained transformers\nIRCAN: mitigating knowledge conflicts in LLM generation via identifying and reweighting context-aware neurons\nDeepDecipher: Accessing and Investigating Neuron Activation in Large Language Models（Neuron-based）\nNeuron to Graph (N2G): Interpreting Language Model Neurons at Scale（Neuron-based）\n\n\nMLLM\nMMNeuron: Discovering Neuron-Level Domain-Specific Interpretation in Multimodal Large Language Model（Neuron-based）\nFinding and editing multi-modal neurons in pre-trained transformers（Neuron-based）\nTowards Neuron Attributions in Multi-Modal Large Language Models (NAM)\nMultimodal neurons in artificial neural networks（Neuron-based）\nMultimodal neurons in pretrained text-only transformers（Neuron-based）\nMiner: Mining the underlying pattern of modality-specific neurons in multimodal large language models（Neuron-based）\nInterpreting the second-order effects of neurons in clip（Neuron-based）\nIdentifying Multi-modal Knowledge Neurons in Pretrained Transformers via Two-stage Filtering\nDeciphering Functions of Neurons in Vision-Language Models","layout":null},"children":[]}]},{"data":{"id":"dg0qx3kbvwg0","created":1769656627625,"text":"Decomposition-based Explanation(15)","layout":null,"expandState":"collapse"},"children":[{"data":{"id":"dg0wwvnctgg0","created":1769673537053,"text":"LLM\nAttention-Aware Layer-Wise Relevance Propagation for Transformers\nXai for transformers: Better explanations through conservative propagation\nTransformer interpretability beyond attention visualization\n\n\nMLLM\nLayer-Wise Modality Decomposition for Interpretable Multimodal Sensor Fusion\nInterpreting clip’s image representation via text-based decomposition\nExplaining Similarity in Vision-Language Encoders with Weighted Banzhaf Interactions（博弈论方法倒推贡献，黑盒）\nResiDual Transformer Alignment with Spectral Decomposition（在CLIP上对残差单元进行谱分解)\nQuantifying Structure in CLIP Embeddings: A Statistical Framework for Concept Interpretation（将嵌入矩阵分解为载荷（loadings）和概念向量）\nDecomposing and Interpreting Image Representations via Text in ViTs Beyond CLIP\nInterpreting ResNet-based CLIP via Neuron-Attention Decomposition 将模型输出直接分摊到内部组件（神经元、Attention Head）和输入维度（Token/Patch）\n\n\nOthers\nLayer-wise Relevance Propagation for Neural Networks with Local Renormalization Layers（LRP）\nVisualizing and understanding neural machine translation（LRP）\nExplaining nonlinear classification decisions with deep taylor decomposition（DTD）\nInterpretability in Parameter Space: Minimizing Mechanistic Description Length with Attribution-based Parameter Decomposition（APD，仅适用于toy models）\nQuantifying Model Complexity via Functional Decomposition for Better Post-hoc Interpretability（metric）","layout":null},"children":[]}]},{"data":{"id":"dg0qx2t5bs80","created":1769656625981,"text":"Gradient-based Explanation(23)","layout":null,"expandState":"collapse"},"children":[{"data":{"id":"dg0w20ymt5s0","created":1769671119330,"text":"Others\nAxiomatic Attribution for Deep Networks (Integrated Gradients)\nDiscretized Integrated Gradients for Explaining Language Models（DIG）\nGuided integrated gradients: An adaptive path method for removing noise\nGrad-CAM: Visual Explanatio ns from Deep Networks via Gradient-based Localization（Grad-CAM）\nGrad-CAM++: Generalized Gradient-Based Visual Explanations for Deep Convolutional Networks（Grad-CAM++）\nSmooth Grad-CAM++: An Enhanced Inference Level Visualization Technique for Deep Convolutional Neural Network Models\nSmoothGrad: removing noise by adding noise（SmoothGrad）\n\n\nSurvey\nGradient based Feature Attribution in Explainable AI: A Technical Review（metric）\n\n\nLLM\nGrad-ELLM: Gradient-based Explanations for Decoder-only LLMs\nSequential Integrated Gradients: a simple but effective method for explaining language models\nAnalyzing Chain-of-Thought Prompting in Large Language Models via Gradient-based Feature Attributions\nDecoding Layer Saliency in Language Transformers\nWhat happened in LLMs layers when trained for fast vs. slow thinking: A gradient perspective\nEnhancing large language model performance with gradient-based parameter selection\n\n\nMLLM\nQ-GroundCAM: Quantifying Grounding in Vision Language Models via GradCAM\nGradient-based Visual Explanation for Transformer-based CLIP\nGrad-ECLIP: Gradient-based Visual and Textual Explanations for CLIP\nImproving Visual Grounding by Encouraging Consistent Gradient-based Explanations\nFrom Redundancy to Relevance: Information Flow in LVLMs Across Reasoning Tasks（LLaVA-CAM + attention score）\nMitigating Multimodal Hallucinations via Gradient-based Self-Reflection\nGLIMPSE: Holistic Cross-Modal Explainability for Large Vision-Language Models\nGrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs and VLMs\nMulti-Modal Interpretability for Enhanced Localization in Vision-Language Models","layout":null},"children":[]}]},{"data":{"id":"dg0qx4gy1aw0","created":1769656629597,"text":"Logit Lens(10)","layout":null,"expandState":"collapse"},"children":[{"data":{"id":"dg0x0t8hryg0","created":1769673845257,"text":"LLM\nInterpreting GPT: the logit lens（Logit Lens）\nAnalyzing Transformers in Embedding Space（Logit Lens）\nTransformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space\nEliciting Latent Predictions from Transformers with the Tuned Lens（Tuned Lens）\nLogitLens4LLMs: Extending Logit Lens Analysis to Modern Large Language Models\nLayered Bias: Interpreting Bias in Pretrained Large Language Models\n\n\nMLLM\nBeyond Logit Lens: Contextual Embeddings for Robust Hallucination Detection & Grounding in VLMs（Logit Lens）\nTowards interpreting visual information processing in vision-language models（Logit Lens）\nInterpreting and Editing Vision-Language Representations to Mitigate Hallucinations（Logit Lens）\nContext-Aware Decoding for Faithful Vision-Language Generation","layout":null},"children":[]}]},{"data":{"id":"dg0qx5mirz40","created":1769656632111,"text":"Sparse Autoencoders(19)","layout":null,"expandState":"collapse"},"children":[{"data":{"id":"dg0x143d6eo0","created":1769673868892,"text":"Survey\nA Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of Large Language Models（metric）\n\n\nLLM\nTowards Monosemanticity: Decomposing Language Models With Dictionary Learning\nScaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet\nScaling and evaluating sparse autoencoders\nJumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse Autoencoders\nThe Geometry of Concepts: Sparse Autoencoder Feature Structure\nMechanistic Interpretability of Code Correctness in LLMs via Sparse Autoencoders（SAE）\nSparse Autoencoders Find Highly Interpretable Features in Language Models（SAE）\nGemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2（SAE）\nLlama Scope: Extracting Millions of Features from Llama-3.1-8B with Sparse Autoencoders（SAE）\n\n\nMLLM\nInterpreting CLIP with Hierarchical Sparse Autoencoders（VL-SAE）\nLarge Multi-modal Models Can Interpret Features in Large Multi-modal Models（VL-SAE）\nSAE-V: Interpreting Multimodal Models for Enhanced Alignment（VL-SAE）\nVL-SAE: Interpreting and Enhancing Vision-Language Alignment with a Unified Concept Set（VL-SAE）\nSparse Autoencoders Learn Monosemantic Features in  Vision-Language Models（VL-SAE）\nSparse Autoencoders for Scientifically Rigorous Interpretation of Vision Models（VL-SAE）\nCausal Interpretation of Sparse Autoencoder Features in Vision（VL-SAE）\nSAVE: Sparse Autoencoder-Driven Visual Information Enhancement for Mitigating Object Hallucination（VL-SAE）\nTextual Steering Vectors Can Improve Visual Understanding in Multimodal Large Language Models","layout":null},"children":[]}]},{"data":{"id":"dg0qx6ntm4g0","created":1769656634366,"text":"Causal Tracing","layout":null,"expandState":"expand"},"children":[]},{"data":{"id":"dg0qx7h3dko0","created":1769656636136,"text":"Circuit Discovery","layout":null,"expandState":"expand"},"children":[]}]}]},{"data":{"id":"dg0spmhtaao0","created":1769661684138,"text":"Evaluation(§4)","layout":null,"expandState":"expand","font-weight":"bold"},"children":[{"data":{"id":"dg0sqc2kfps0","created":1769661739812,"text":"Evaluation Criteria","layout":null,"expandState":"expand"},"children":[{"data":{"id":"dg0sqljhfyo0","created":1769661760426,"text":"Faithfulness","layout":null,"expandState":"expand"},"children":[]},{"data":{"id":"dg0ss1tbkz40","created":1769661874214,"text":"Stability","layout":null,"expandState":"expand"},"children":[]},{"data":{"id":"dg0y1qp6ck00","created":1769676739210,"text":"Consistency","layout":null},"children":[]},{"data":{"id":"dg0y1x8f0400","created":1769676753434,"text":"Generalizability","layout":null},"children":[]},{"data":{"id":"dg0ss5flp8g0","created":1769661882091,"text":"Efficiency","layout":null,"expandState":"expand"},"children":[]}]},{"data":{"id":"dg0sqfsu50g0","created":1769661747931,"text":"Benchmarks","layout":null,"expandState":"expand"},"children":[]},{"data":{"id":"dg0stet6cts0","created":1769661980867,"text":"Toolkits & Infrastructure","layout":null,"expandState":"expand"},"children":[]}]},{"data":{"id":"dg0qv1divzk0","created":1769656466131,"text":"Applications(§5)","layout":null,"expandState":"expand","font-weight":"bold"},"children":[{"data":{"id":"dg0qzxz97b40","created":1769656850559,"text":"Medical Vision-Language Systems","layout":null,"expandState":"expand"},"children":[]},{"data":{"id":"dg0qzz9nqp40","created":1769656853365,"text":"Jailbreak & Safety Analysis","layout":null,"expandState":"expand"},"children":[]},{"data":{"id":"dg0r015vm9k0","created":1769656857490,"text":"Harmful Content Detection","layout":null,"expandState":"expand"},"children":[]},{"data":{"id":"dg0y2zl2ng00","created":1769676836917,"text":"Embodied AI & Robotics"},"children":[]},{"data":{"id":"dg0y3nbpq5c0","created":1769676888594,"text":"Autonomous Driving"},"children":[]}]},{"data":{"id":"dg0qnudnka00","created":1769655902352,"text":"Challenges and Future Directions(§6)","layout":null,"expandState":"expand","font-weight":"bold"},"children":[{"data":{"id":"dg0sv6578sw0","created":1769662118732,"text":"Bridging the Semantic Gap Across Modalities","layout":null,"expandState":"expand"},"children":[]},{"data":{"id":"dg0y7mo5t0g0","created":1769677200626,"text":"Scaling Mechanistic Interpretability to Vision–Language Models"},"children":[]},{"data":{"id":"dg0y7nko8yo0","created":1769677202592,"text":"Towards Human-Centered Evaluation of Multimodal Interpretability"},"children":[]},{"data":{"id":"dg0y7li3qhc0","created":1769677198083,"text":"From Standalone Methods to Interpretability Systems"},"children":[]}]}]},"template":"right","theme":"fresh-blue","version":"1.4.43"}